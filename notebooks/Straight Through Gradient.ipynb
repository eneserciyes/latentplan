{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ed86e47-5fbf-4d96-a029-4fb691c8db21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2936c9f5-4df4-4e19-8571-9b3ef8bbf188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c81eb0c-b77e-41d0-bdd3-96aba7e08f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantization(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, codebook):\n",
    "        with torch.no_grad():\n",
    "            embedding_size = codebook.size(1)\n",
    "            inputs_size = inputs.size()\n",
    "            inputs_flatten = inputs.view(-1, embedding_size)\n",
    "\n",
    "            codebook_sqr = torch.sum(codebook ** 2, dim=1)\n",
    "            inputs_sqr = torch.sum(inputs_flatten ** 2, dim=1, keepdim=True)\n",
    "\n",
    "            # Compute the distances to the codebook\n",
    "            distances = torch.addmm(codebook_sqr + inputs_sqr,\n",
    "                inputs_flatten, codebook.t(), alpha=-2.0, beta=1.0)\n",
    "\n",
    "            _, indices_flatten = torch.min(distances, dim=1)\n",
    "            indices = indices_flatten.view(*inputs_size[:-1])\n",
    "            ctx.mark_non_differentiable(indices)\n",
    "\n",
    "            return indices\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        raise RuntimeError('Trying to call `.grad()` on graph containing '\n",
    "            '`VectorQuantization`. The function `VectorQuantization` '\n",
    "            'is not differentiable. Use `VectorQuantizationStraightThrough` '\n",
    "            'if you want a straight-through estimator of the gradient.')\n",
    "\n",
    "class VectorQuantizationStraightThrough(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, codebook):\n",
    "        indices = vq(inputs, codebook)\n",
    "        indices_flatten = indices.view(-1)\n",
    "        ctx.save_for_backward(indices_flatten, codebook)\n",
    "        ctx.mark_non_differentiable(indices_flatten)\n",
    "\n",
    "        codes_flatten = torch.index_select(codebook, dim=0,\n",
    "            index=indices_flatten)\n",
    "        codes = codes_flatten.view_as(inputs)\n",
    "\n",
    "        return (codes, indices_flatten)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output, grad_indices):\n",
    "        grad_inputs, grad_codebook = None, None\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            # Straight-through estimator\n",
    "            grad_inputs = grad_output.clone()\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            # Gradient wrt. the codebook\n",
    "            indices, codebook = ctx.saved_tensors\n",
    "            embedding_size = codebook.size(1)\n",
    "\n",
    "            grad_output_flatten = (grad_output.contiguous()\n",
    "                                              .view(-1, embedding_size))\n",
    "            grad_codebook = torch.zeros_like(codebook)\n",
    "            grad_codebook.index_add_(0, indices, grad_output_flatten)\n",
    "\n",
    "        return (grad_inputs, grad_codebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5be110ac-fc58-46ee-bbaf-86b6bae38440",
   "metadata": {},
   "outputs": [],
   "source": [
    "vq = VectorQuantization.apply\n",
    "vq_st = VectorQuantizationStraightThrough.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12631933-3cb4-4dcf-a067-0125184fa020",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQEmbedding(nn.Module):\n",
    "    def __init__(self, K, D):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(K, D)\n",
    "        self.embedding.weight.data.uniform_(-1./K, 1./K)\n",
    "\n",
    "    def forward(self, z_e_x):\n",
    "        z_e_x_ = z_e_x.contiguous()\n",
    "        latents = vq(z_e_x_, self.embedding.weight)\n",
    "        return latents\n",
    "\n",
    "    def straight_through(self, z_e_x):\n",
    "        z_e_x_ = z_e_x.contiguous()\n",
    "        z_q_x_, indices = vq_st(z_e_x_, self.embedding.weight)\n",
    "        z_q_x = z_q_x_.contiguous()\n",
    "\n",
    "        z_q_x_bar_flatten = torch.index_select(self.embedding.weight,\n",
    "            dim=0, index=indices)\n",
    "        z_q_x_bar_ = z_q_x_bar_flatten.view_as(z_e_x_)\n",
    "        z_q_x_bar = z_q_x_bar_.contiguous()\n",
    "\n",
    "        return z_q_x, z_q_x_bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c6957ba-8c97-4560-a309-ed69752a32ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1017, -0.0756, -0.0260,  0.0423],\n",
      "        [ 0.0249, -0.0127, -0.0857, -0.0423],\n",
      "        [-0.0179, -0.1194, -0.0493,  0.0043],\n",
      "        [ 0.1053,  0.0444,  0.0938, -0.0371],\n",
      "        [-0.1228, -0.0994,  0.0341,  0.0482],\n",
      "        [ 0.0325, -0.0910,  0.0846,  0.0073],\n",
      "        [ 0.0806,  0.0586, -0.0904, -0.0681],\n",
      "        [ 0.1024, -0.0948,  0.1128, -0.0157]], requires_grad=True)\n",
      "Latent code shape: torch.Size([1, 2, 4])\n",
      "Latents_st: tensor([[[ 0.0249, -0.0127, -0.0857, -0.0423],\n",
      "         [ 0.1053,  0.0444,  0.0938, -0.0371]]],\n",
      "       grad_fn=<VectorQuantizationStraightThroughBackward>)\n",
      "Latents: tensor([[[ 0.0249, -0.0127, -0.0857, -0.0423],\n",
      "         [ 0.1053,  0.0444,  0.0938, -0.0371]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[2., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.]]])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [2., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    codebook = VQEmbedding(K=8, D=4)\n",
    "    codebook.requires_grad = True\n",
    "    print(codebook.embedding.weight)\n",
    "    \n",
    "    trajectory_feature = torch.zeros(1,2,4)\n",
    "    trajectory_feature[0,1,:].fill_(1)\n",
    "    trajectory_feature.requires_grad = True\n",
    "    latents_st, latents = codebook.straight_through(trajectory_feature)\n",
    "    print(\"Latent code shape:\", latents_st.shape)\n",
    "    print(\"Latents_st:\",  latents_st)\n",
    "    print(\"Latents:\", latents)\n",
    "    loss = 2 * latents_st[0,0,0] + latents_st[0,1,1]\n",
    "    loss.backward()\n",
    "    print(trajectory_feature.grad)\n",
    "    print(codebook.embedding.weight.grad)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa140afa-34b5-4d75-a02e-544717defbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2]])\n"
     ]
    }
   ],
   "source": [
    "def vq_test():\n",
    "    codebook = VQEmbedding(K=4, D=4)\n",
    "    \n",
    "    trajectory_feature = torch.zeros(1,1,4)\n",
    "    trajectory_feature.requires_grad = True\n",
    "    indices = vq(trajectory_feature, codebook.embedding.weight.detach())\n",
    "    print(indices)\n",
    "vq_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bde6b3d-15e8-4b93-a0ff-68d6cc388f54",
   "metadata": {},
   "source": [
    "# VQEmbeddingMovingAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd0f9d0c-e70e-4ca9-a041-365ec0c1f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VQEmbeddingMovingAverage(nn.Module):\n",
    "    def __init__(self, K, D, decay=0.99):\n",
    "        super().__init__()\n",
    "        embedding = torch.zeros(K, D)\n",
    "        embedding.uniform_(-1./K, 1./K)\n",
    "        self.decay = decay\n",
    "\n",
    "        self.register_buffer(\"embedding\", embedding)\n",
    "        self.register_buffer(\"ema_count\", torch.ones(K))\n",
    "        self.register_buffer(\"ema_w\", self.embedding.clone())\n",
    "\n",
    "    def forward(self, z_e_x):\n",
    "        z_e_x_ = z_e_x.contiguous()\n",
    "        latents = vq(z_e_x_, self.embedding.weight)\n",
    "        return latents\n",
    "\n",
    "    def straight_through(self, z_e_x):\n",
    "        ipdb.set_trace()\n",
    "        K, D = self.embedding.size()\n",
    "\n",
    "        z_e_x_ = z_e_x.contiguous()\n",
    "        z_q_x_, indices = vq_st(z_e_x_, self.embedding)\n",
    "        z_q_x = z_q_x_.contiguous()\n",
    "\n",
    "\n",
    "        if self.training:\n",
    "            encodings = F.one_hot(indices, K).float()\n",
    "            self.ema_count = self.decay * self.ema_count + (1 - self.decay) * torch.sum(encodings, dim=0)\n",
    "\n",
    "            dw = encodings.transpose(1, 0)@z_e_x_.reshape([-1, D])\n",
    "            self.ema_w = self.decay * self.ema_w + (1 - self.decay) * dw\n",
    "\n",
    "            self.embedding = self.ema_w / (self.ema_count.unsqueeze(-1))\n",
    "            self.embedding = self.embedding.detach()\n",
    "            self.ema_w = self.ema_w.detach()\n",
    "            self.ema_count = self.ema_count.detach()\n",
    "\n",
    "        z_q_x_bar_flatten = torch.index_select(self.embedding, dim=0, index=indices)\n",
    "        z_q_x_bar_ = z_q_x_bar_flatten.view_as(z_e_x_)\n",
    "        z_q_x_bar = z_q_x_bar_.contiguous()\n",
    "\n",
    "        return z_q_x, z_q_x_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e4037-f080-4151-8e42-006884ca9e3e",
   "metadata": {},
   "source": [
    "# Grad test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d82cc80-2d0f-42be-94c6-5564a025feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "function test_grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
