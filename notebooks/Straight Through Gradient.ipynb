{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ed86e47-5fbf-4d96-a029-4fb691c8db21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2936c9f5-4df4-4e19-8571-9b3ef8bbf188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import ipdb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c81eb0c-b77e-41d0-bdd3-96aba7e08f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantization(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, codebook):\n",
    "        with torch.no_grad():\n",
    "            embedding_size = codebook.size(1)\n",
    "            inputs_size = inputs.size()\n",
    "            inputs_flatten = inputs.view(-1, embedding_size)\n",
    "\n",
    "            codebook_sqr = torch.sum(codebook ** 2, dim=1)\n",
    "            inputs_sqr = torch.sum(inputs_flatten ** 2, dim=1, keepdim=True)\n",
    "\n",
    "            # Compute the distances to the codebook\n",
    "            distances = torch.addmm(codebook_sqr + inputs_sqr,\n",
    "                inputs_flatten, codebook.t(), alpha=-2.0, beta=1.0)\n",
    "\n",
    "            _, indices_flatten = torch.min(distances, dim=1)\n",
    "            indices = indices_flatten.view(*inputs_size[:-1])\n",
    "            ctx.mark_non_differentiable(indices)\n",
    "\n",
    "            return indices\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        raise RuntimeError('Trying to call `.grad()` on graph containing '\n",
    "            '`VectorQuantization`. The function `VectorQuantization` '\n",
    "            'is not differentiable. Use `VectorQuantizationStraightThrough` '\n",
    "            'if you want a straight-through estimator of the gradient.')\n",
    "\n",
    "class VectorQuantizationStraightThrough(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, codebook):\n",
    "        indices = vq(inputs, codebook)\n",
    "        indices_flatten = indices.view(-1)\n",
    "        ctx.save_for_backward(indices_flatten, codebook)\n",
    "        ctx.mark_non_differentiable(indices_flatten)\n",
    "\n",
    "        codes_flatten = torch.index_select(codebook, dim=0,\n",
    "            index=indices_flatten)\n",
    "        codes = codes_flatten.view_as(inputs)\n",
    "\n",
    "        return (codes, indices_flatten)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output, grad_indices):\n",
    "        grad_inputs, grad_codebook = None, None\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            # Straight-through estimator\n",
    "            grad_inputs = grad_output.clone()\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            # Gradient wrt. the codebook\n",
    "            indices, codebook = ctx.saved_tensors\n",
    "            embedding_size = codebook.size(1)\n",
    "\n",
    "            grad_output_flatten = (grad_output.contiguous()\n",
    "                                              .view(-1, embedding_size))\n",
    "            grad_codebook = torch.zeros_like(codebook)\n",
    "            grad_codebook.index_add_(0, indices, grad_output_flatten)\n",
    "\n",
    "        return (grad_inputs, grad_codebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5be110ac-fc58-46ee-bbaf-86b6bae38440",
   "metadata": {},
   "outputs": [],
   "source": [
    "vq = VectorQuantization.apply\n",
    "vq_st = VectorQuantizationStraightThrough.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12631933-3cb4-4dcf-a067-0125184fa020",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQEmbedding(nn.Module):\n",
    "    def __init__(self, K, D):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(K, D)\n",
    "        self.embedding.weight.data.uniform_(-1./K, 1./K)\n",
    "\n",
    "    def forward(self, z_e_x):\n",
    "        z_e_x_ = z_e_x.contiguous()\n",
    "        latents = vq(z_e_x_, self.embedding.weight)\n",
    "        return latents\n",
    "\n",
    "    def straight_through(self, z_e_x):\n",
    "        z_e_x_ = z_e_x.contiguous()\n",
    "        z_q_x_, indices = vq_st(z_e_x_, self.embedding.weight)\n",
    "        z_q_x = z_q_x_.contiguous()\n",
    "\n",
    "        z_q_x_bar_flatten = torch.index_select(self.embedding.weight,\n",
    "            dim=0, index=indices)\n",
    "        z_q_x_bar_ = z_q_x_bar_flatten.view_as(z_e_x_)\n",
    "        z_q_x_bar = z_q_x_bar_.contiguous()\n",
    "\n",
    "        return z_q_x, z_q_x_bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd0f9d0c-e70e-4ca9-a041-365ec0c1f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VQEmbeddingMovingAverage(nn.Module):\n",
    "    def __init__(self, K, D, decay=0.99):\n",
    "        super().__init__()\n",
    "        embedding = torch.zeros(K, D)\n",
    "        embedding.uniform_(-1./K, 1./K)\n",
    "        self.decay = decay\n",
    "\n",
    "        self.register_buffer(\"embedding\", embedding)\n",
    "        self.register_buffer(\"ema_count\", torch.ones(K))\n",
    "        self.register_buffer(\"ema_w\", self.embedding.clone())\n",
    "\n",
    "    def forward(self, z_e_x):\n",
    "        z_e_x_ = z_e_x.contiguous()\n",
    "        latents = vq(z_e_x_, self.embedding.weight)\n",
    "        return latents\n",
    "\n",
    "    def straight_through(self, z_e_x):\n",
    "        ipdb.set_trace()\n",
    "        K, D = self.embedding.size()\n",
    "\n",
    "        z_e_x_ = z_e_x.contiguous()\n",
    "        z_q_x_, indices = vq_st(z_e_x_, self.embedding)\n",
    "        z_q_x = z_q_x_.contiguous()\n",
    "\n",
    "\n",
    "        if self.training:\n",
    "            encodings = F.one_hot(indices, K).float()\n",
    "            self.ema_count = self.decay * self.ema_count + (1 - self.decay) * torch.sum(encodings, dim=0)\n",
    "\n",
    "            dw = encodings.transpose(1, 0)@z_e_x_.reshape([-1, D])\n",
    "            self.ema_w = self.decay * self.ema_w + (1 - self.decay) * dw\n",
    "\n",
    "            self.embedding = self.ema_w / (self.ema_count.unsqueeze(-1))\n",
    "            self.embedding = self.embedding.detach()\n",
    "            self.ema_w = self.ema_w.detach()\n",
    "            self.ema_count = self.ema_count.detach()\n",
    "\n",
    "        z_q_x_bar_flatten = torch.index_select(self.embedding, dim=0, index=indices)\n",
    "        z_q_x_bar_ = z_q_x_bar_flatten.view_as(z_e_x_)\n",
    "        z_q_x_bar = z_q_x_bar_.contiguous()\n",
    "\n",
    "        return z_q_x, z_q_x_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c6957ba-8c97-4560-a309-ed69752a32ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0019, -0.0002,  0.0009,  ...,  0.0015,  0.0009,  0.0014],\n",
      "        [ 0.0007, -0.0005, -0.0015,  ..., -0.0017, -0.0005, -0.0014],\n",
      "        [ 0.0016,  0.0018,  0.0009,  ..., -0.0018,  0.0018,  0.0007],\n",
      "        ...,\n",
      "        [ 0.0004, -0.0017, -0.0016,  ..., -0.0012,  0.0002, -0.0004],\n",
      "        [-0.0015,  0.0014, -0.0008,  ..., -0.0007,  0.0016, -0.0016],\n",
      "        [ 0.0014, -0.0012,  0.0005,  ...,  0.0006, -0.0003,  0.0015]])\n",
      "> \u001b[0;32m/var/folders/js/rf57f6s5077gn5pslm2v3lsh0000gn/T/ipykernel_92234/6531829.py\u001b[0m(19)\u001b[0;36mstraight_through\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     18 \u001b[0;31m        \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 19 \u001b[0;31m        \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     20 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "tensor(1.1151e-06, grad_fn=<AddBackward0>)\n",
      "tensor([[[-0.0013,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000, -0.0005,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.0018,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    weights = torch.load(\"../../latentplan.jl/latentplan/test/files/gpt_weights.pt\")\n",
    "    codebook = VQEmbeddingMovingAverage(K=512, D=512)\n",
    "    codebook.embedding = weights[\"model.codebook.embedding\"]\n",
    "    codebook.ema_count = weights[\"model.codebook.ema_count\"]\n",
    "    codebook.ema_w = weights[\"model.codebook.ema_w\"]\n",
    "\n",
    "    codebook.requires_grad = True\n",
    "    print(codebook.embedding)\n",
    "    \n",
    "    trajectory_feature = torch.tensor(np.load(\"../../latentplan.jl/latentplan/test/files/trajectory_feature.npy\"))\n",
    "    trajectory_feature.requires_grad = True\n",
    "\n",
    "    latents_st, latents = codebook.straight_through(trajectory_feature)\n",
    "    loss = 2 * latents_st[0,0,0] ** 2 + latents_st[0,1,1] ** 2 + latents_st[0,2,2] ** 2\n",
    "    loss.backward()\n",
    "    print(loss)\n",
    "    np.save(\"trajectory_feature_straight_through_grad.npy\", trajectory_feature.grad.cpu().detach().numpy())\n",
    "    \n",
    "    print(trajectory_feature.grad)\n",
    "    print(codebook.embedding.grad)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d09957cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.load(\"trajectory_feature_straight_through_grad.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa0aafc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        , -0.00183442,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0,:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa140afa-34b5-4d75-a02e-544717defbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0]])\n"
     ]
    }
   ],
   "source": [
    "def vq_test():\n",
    "    codebook = VQEmbedding(K=4, D=4)\n",
    "    \n",
    "    trajectory_feature = torch.zeros(1,1,4)\n",
    "    trajectory_feature.requires_grad = True\n",
    "    indices = vq(trajectory_feature, codebook.embedding.weight.detach())\n",
    "    print(indices)\n",
    "vq_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bde6b3d-15e8-4b93-a0ff-68d6cc388f54",
   "metadata": {},
   "source": [
    "# VQEmbeddingMovingAverage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tap')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9f7395b894a795d186fa6f195b74b66ad67a3101d96dc83d400460a82c54db1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
